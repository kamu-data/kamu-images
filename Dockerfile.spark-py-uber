ARG SPARK_VERSION
FROM kamudata/spark-py:$SPARK_VERSION


# Tools
RUN apk add jq
RUN wget -O /usr/local/bin/yq "https://github.com/mikefarah/yq/releases/download/2.4.1/yq_linux_amd64" && \
    chmod +x /usr/local/bin/yq


# Logging
ADD spark/log4j.properties /opt/spark/conf/log4j.properties


# GeoSpark
RUN cd /opt/spark/jars && \
    rm -f /opt/spark/jars/geospark-*.jar && \
    wget https://github.com/DataSystemsLab/GeoSpark/releases/download/1.2.0-spark-2.3/geospark-1.2.0.jar && \
    wget https://github.com/DataSystemsLab/GeoSpark/releases/download/1.2.0-spark-2.3/geospark-sql_2.3-1.2.0.jar

ADD spark/spark-defaults.conf /opt/spark/conf/spark-defaults.conf


# Livy
# TODO: Until Spark or Livy have a mechanism for auto-registering custom
# UDTs and UDFs we are forced to fork Livy to register GeoSpark types.
# RUN cd /opt && \
#     wget http://apache.forsale.plus/incubator/livy/0.6.0-incubating/apache-livy-0.6.0-incubating-bin.zip && \
#     unzip apache-livy*.zip && rm apache-livy*.zip && mv apache-livy* livy && \
#     mkdir /opt/livy/logs

ADD livy/apache-livy-0.7.0-kamu-bin.zip /opt
RUN cd /opt && \
    unzip apache-livy*.zip && rm apache-livy*.zip && mv apache-livy* livy && \
    mkdir /opt/livy/logs

ADD livy/log4j.properties /opt/livy/conf/log4j.properties
ADD livy/livy.conf /opt/livy/conf/livy.conf


ADD livy/entrypoint.sh /opt/entrypoint.sh
